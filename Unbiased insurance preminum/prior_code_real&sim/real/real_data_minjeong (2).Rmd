---
title: "Real Data Analysis"
author: "232STG08 박민정"
output: html_document
---


```{r}
source("repeated_data_management.R") 

summary(data.train)
dim(data)
dim(dataout)
dim(data.train)
dim(data.valid)

data.train$C1  <- ifelse(data.train$col.Cov.Idx==1, 1, 0)
data.train$C2  <- ifelse(data.train$col.Cov.Idx==2, 1, 0)
data.valid$C1  <- ifelse(data.valid$col.Cov.Idx==1, 1, 0)
data.valid$C2  <- ifelse(data.valid$col.Cov.Idx==2, 1, 0)
```



### Task 1) beta_hat, alpha_hat, p_hat 추정

- 설명변수 선택 --> 오로지 교수님 논문 세팅과 동일하게

```{r}
uniq_pol = unique(data.train$PolicyNum)

person=c(NULL)
for(i in 1:length(uniq_pol)){
  ind = uniq_pol[i] == data.train$PolicyNum 
  person=c(person, rep(i, sum(ind)))
}
person    
max(person)
length(uniq_pol)


library(nimble)
library(MCMCvis)


X = model.matrix(n~TypeCity+TypeCounty+TypeSchool+TypeTown+TypeVillage+C1+C2, data = data.train)


model <- nimbleCode({
  # likelihood
  for(j in 1:JJ){
    lambda[j] <-exp(inprod(X[j,1:KK], beta.hat[1:KK]))
    y[j] ~ dnegbin(prob = 2/(2+lambda[j]*R[Person[j]]), size = 2)
  }
  
  # prior of R
  for(n in 1:N.person){
    R[n] ~ dgamma(mean=1, sd=1/a.hat)
  }
  
  # prior of Betas (params 개수 만큼 추출)
  for(j in 1:KK){
    beta.hat[j] ~ dnorm(mean=0, sd=10)
  } 
  
  # prior of alpha
  a.hat ~ dunif(min=0, max=10)
  
})

my.data <- list(X = X, y = data.train$n)
my.constants <- list(JJ=dim(X)[1], KK=dim(X)[2], Person=person, N.person=max(person))
parameters.to.save <- c("beta.hat", "a.hat", "lambda", "R")

pois_model <- nimbleModel(code = model, name = "pois_model", constants = my.constants, data = my.data)
Cpois_model  <- compileNimble(pois_model)
pois_modelConf <- configureMCMC(pois_model, print = TRUE)
pois_modelConf$addMonitors('beta.hat', "a.hat", "lambda", "R")
MCMC <- buildMCMC(pois_modelConf)
CMCMC <- compileNimble(MCMC, project = pois_model)


n.iter <- 20000
n.burnin <- 4000
n.chains <- 2

set.seed(123)
samples <- runMCMC(CMCMC, niter = n.iter,
                   nburnin = n.burnin,
                   nchains = n.chains)

MCMCtrace(object = samples,
          pdf = FALSE, # no export to PDF
          ind = TRUE, # separate density lines per chain
          params = "beta.hat")

MCMCtrace(object = samples,
          pdf = FALSE, # no export to PDF
          ind = TRUE, # separate density lines per chain
          params = "a.hat")


str(samples[[1]])
result <- MCMCsummary(object = samples, round = 2)
dim(result)

# save estimated params
R <- result[1:409,1] ;R #409
a.hat <- result[410,1] ;a.hat #1
beta.hat <-  result[411:418,1] ;beta.hat #8
lambda.hat <- result[419:1885,1] #1467
summary(lambda.hat)

lambda_uq = unique(cbind(person, lambda.hat))
```

- Visualization of lambda_hat 
```{r}
library(tidyverse)

lambdas <- data.frame(lambda.hat)
table(lambda.hat) # 17 levels ... 

ggplot(lambdas,aes(lambda.hat))+geom_bar()
```





### Task 2) Comparing MSE & Biasedness of 3 Models

##함수 정의
```{r}

# -------------------------------------------------------------------------------
# 필요 함수 정의 (proposition coeff, Yhat)
# -------------------------------------------------------------------------------

# 각 모델의 optm값( 즉 model3의 경우 alpha0, alpha1 // model2의 경우 alpha0) 계산에 이용되는 값 계산
# E_ysq, E_lam_ysq, E_lamsq_ysq, E_lamsq_ybar, E_lamsq_ybarsq, E_lam_y, E_lamsq_y, E_lam_y_ybar, E_lamsq_y_ybar
prop_coeff <- function(lam_vec, a, tau, k){
  #E_ysq = var(u_lam_R) + mean(v_lam_R) + mean(u_lam_R)^2
  E_lam_vec    = mean(lam_vec)
  E_lam_vec_sq = mean(lam_vec^2)
  E_lam_vec_th = mean(lam_vec^3)
  E_lam_vec_for = mean(lam_vec^4)
  
  E_ysq0          = E_lam_vec_sq + E_lam_vec + 1/k * E_lam_vec_sq * (1/a+1)
  E_lam_ysq0      = E_lam_vec_sq + 1/k * E_lam_vec_th * (1+1/a) + E_lam_vec_sq
  E_lamsq_ysq0    = E_lam_vec_th + 1/k * E_lam_vec_for * (1+1/a) + E_lam_vec_th
  E_lamsq_ybar0   = E_lam_vec_th
  E_lamsq_ybarsq0 = 1/tau * E_lam_vec_th + 1/(2*k) * E_lam_vec_for * (1+1/a) + E_lam_vec_for * (1+1/a)
  E_lam_y0       = E_lam_vec_sq
  E_lamsq_y0      = E_lam_vec_th
  E_lam_y_ybar0   = E_lam_vec_th * (1/a + 1)
  E_lamsq_y_ybar0 = E_lam_vec_for * (1+1/a)
  E_lam_ybarsq0 = 1/tau * E_lam_vec_sq + 1/(2*k) * E_lam_vec_th * (1+1/a) + E_lam_vec_th * (1+1/a)
  E_lamth_ybar0 = E_lam_vec_for
  E_lamth_y0    = E_lam_vec_for
  
  return (c(E_ysq = E_ysq0, E_lam_ysq = E_lam_ysq0, E_lamsq_ysq = E_lamsq_ysq0, E_lamsq_ybar = E_lamsq_ybar0, 
            E_lamsq_ybarsq = E_lamsq_ybarsq0, E_lam_y = E_lam_y0, E_lamsq_y = E_lamsq_y0, E_lam_y_ybar = E_lam_y_ybar0,
            E_lamsq_y_ybar = E_lamsq_y_ybar0, E_lam_ybarsq= E_lam_ybarsq0, E_lamth_ybar = E_lamth_ybar0, E_lamth_y = E_lamth_y0))
}


Yhat_ftn <- function( N_sim, n_model, tau, lam_vec, Y, m2_alpha0, m2_alpha1, m3_alpha0){
  Yhat = data.frame(matrix(nrow = N_sim, ncol = n_model))
  
  # model 1,2,3 y_hat값 생성
  for(n in 1: N_sim){
    Yhat[n,1] = lam_vec[n] * (a+sum(Y[n,1:tau]))/(a+tau*lam_vec[n])
    Yhat[n,2] = lam_vec[n] * (m2_alpha0 + m2_alpha1 * sum(Y[n,1:tau])/tau)
    Yhat[n,3] = lam_vec[n]/(m3_alpha0 + (1- m3_alpha0) * lam_vec[n]) * ( m3_alpha0 + (1- m3_alpha0) * sum(Y[n,1:tau])/tau)
  }
  return( Yhat )
}


cal_mse <- function(n_model, y_true = Y[,6]){
  mse = data.frame(matrix(nrow = 1, ncol = n_model))
  for(j in 1: n_model){
    mse[1,j] = mean((Yhat[,j]-y_true)^2)
  }
  return( mse )
}


# 각 모델별 unbiasedness 계산

#lam_pol : PolicyNum 한명의 고유한 lambda.hat (총 409개)
cal_UB <- function(lambs, n_model,Yhat, lam_pol){
  E_y_given_lam <- data.frame(matrix(nrow = length(lambs), ncol = n_model+1))
  
  total <- Yhat
  total$lam_pol <- lam_pol # tmp <- cbind(Yhat, lam_vec)이건 왜 안됨..?
  for( i in 1: length(lambs)){
    E_y_given_lam[i,] = apply(total[total$lam_pol == lambs[i],],2,mean) # 각 모델의 lamb별 평균
  }
  
  total_unbiasedness_ch = apply((E_y_given_lam[,1:n_model]-lambs)^2/lambs,2,sum)
  return(total_unbiasedness_ch)
}
```


## Model 1,2,3 fitting
```{r}
N_sim=length(uniq_pol)
tau = length(unique(data.train$Year)); n_model = 3
k =2 # NB의 param
lam_vec = lambda.hat
a  = a.hat
lambs <- unique(lam_vec)


# Y 구성
Y = data.frame(matrix(nrow = length(uniq_pol), ncol = tau))

for(i in 1:length(uniq_pol)){
  Y[i,tau+1] = uniq_pol[i]
  ys = data.train[data.train$PolicyNum == uniq_pol[i], ]$n
  for(j in 1: length(ys)){
    Y[i,j] = ys[j]
  }
}
#Y$n <- data.valid$n # target은 이름이 n인 열로 구성

colnames(Y) <- c("y_1", "y_2", "y_3",  "y_4", "y_5", "ID")
Y
original_Y = Y


## 결측치 채우기
for (i in 1:length(ID)){
  for (j in 1:tau){
    if (is.na(Y[i,j])==TRUE)
      Y[i,j] = dnbinom(1,prob = 2/(2+lambda_uq[i,2]*R[i]), size = 2)}
}
Y #최종 Y
```



```{r}
# 각 모델에 대한 mod2의 알파0,알파1과 mod3의 알파0 계산
P <- prop_coeff(lam_vec, a, tau, k)

m2_alpha1 = (P["E_lam_y"] * P["E_lamsq_ybar"] - P["E_lam_y_ybar"] * mean(lam_vec^2))       /(P["E_lamsq_ybar"]^2-P["E_lamsq_ybarsq"]*mean(lam_vec^2))
m2_alpha0 = (P["E_lamsq_ybar"] * P["E_lam_y_ybar"] - P["E_lam_y"] * P["E_lamsq_ybarsq"])/(P["E_lamsq_ybar"]^2-P["E_lamsq_ybarsq"]*mean(lam_vec^2))

# m3에 대한 alpha0 
m3_alpha0_n = P["E_lamsq_y_ybar"] - P["E_lamth_y"] - P["E_lamsq_ybarsq"] + P["E_lamth_ybar"]
m3_alpha0_d = P["E_lam_y_ybar"] - P["E_lamsq_y_ybar"] - P["E_lamsq_ybar"] + P["E_lamsq_ybarsq"] - P["E_lamsq_y"]  + P["E_lamth_y"] + mean(lam_vec^3) - P["E_lamth_ybar"]

m3_alpha0 = - m3_alpha0_n/m3_alpha0_d


# 각 모델별 예측값 추출 
Yhat <- Yhat_ftn( N_sim, n_model, tau, lam_vec, Y, m2_alpha0, m2_alpha1, m3_alpha0)

# MSE 계산
mse_model <- cal_mse(n_model, y_true = Y[,tau+1])

# 각 모델별 unbiasedness 확인
total_unbiasedness_ch <- cal_UB(lambs,n_model,Yhat, lam_vec)

total_unbiasedness_ch



E_y_given_lam <- data.frame(matrix(nrow = length(lambs), ncol = n_model+1))
  
  total <- Yhat
  total$lam_vec <- lambs 

for( i in 1: length(lambs)){
    E_y_given_lam[i,] = apply(total[total$lam_vec == lambs[i],],2,mean) # 각 모델의 lamb별 평균
  }
```




#### GLM, GLMM fitting
```{r}
library(lme4)

glm <- glm(n ~ TypeCity+TypeCounty+TypeSchool+TypeTown+TypeVillage+PolicyNum+C1+C2, data = data.train, family=poisson)
pred.glm<-predict(glm, type="response", newdata=data.valid)

glmm <- glmer(n ~ TypeCity+TypeCounty+TypeSchool+TypeTown+TypeVillage+(1|PolicyNum)+C1+C2, data = data.train, family=poisson)
pred.glmm<-predict(glmm, type="response", newdata=data.valid)

#test MSE using regression model
mean((pred.glm- data.valid$n)^2) #glm
mean((pred.glmm- data.valid$n)^2) #glmm
```


